# 카프카 상세 개념

## 토픽과 파티션

### 1. 적정 파티션 개수

**고려사항**

- 데이터 처리량

데이터 처리 속도 올리려면

1. 컨슈머 처리량 올리거나 → 스케일 업(사양 업글) or GC 튜닝 등 : 일정 수준 이상 어려움
2. 컨슈머 추가하여 병렬처리량 늘리거나 → 확실. 파티션 개수 늘리고 컨슈머 추가

컨슈머 데이터 처리량 * 파티션 개수 > 프로듀서 전송 데이터량 → 레코드 수 등 계산하여 정한다

if 컨슈머 데이터 처리량 < 프로듀서 보내는 데이터 → 컨슈머 랙, 데이터 처리 지연

상용환경에서의 테스트, 파티션 개수 늘림에 따른 부담, 데이터 처리 지연이 있어도 되는지 등을 종합적으로 고려

- 메시지 키 사용 여부 (+ 데이터 처리 순서 지켜야 하는지?)

파티션 개수 달라지면 메시지 키 사용하는 컨슈머는 특정 메시지 키의 순서를 더는 보장받지 못함 → 파티션 변환 이전과 이후 메시지 키 파티션 위치가 달라지기 때문

SO, 메시지 키 사용, 메시지 처리 순서가 보장되어야 한다면 파티션 변화 발생하지 않는 방향으로.

변화해야 한다면 메시지 키 매칭을 그대로 사용하기 위해 커스텀 파티셔너 등 개발, 적용

- 브로커, 컨슈머 영향도

파티션 늘어날수록 브로커에서 접근하는 파일 개수 많아짐(각 브로커의 파일 시스템). 하지만 운영체제가 프로세스당 열 수 있는 파일 개수 제한하기 때문에 브로커당 파티션 개수 확인해야 한다. 브로커의 파티션 개수 너무 많아지면 브로커 늘려서 파티션 분산하는 방안도 고려.

### 2. 토픽 정리 정책(cleanup.policy)

토픽의 데이터 시간 또는 용량에 따라 삭제 규칙 적용 가능. 데이터 사용하지 않을 땐 cleanup.policy 옵션 사용

**1) 토픽 삭제 정책(delete policy)**

세그먼트 - 토픽의 데이터 저장하는 명시적인 파일 시스템 단위. 오프셋 중 가장 작은 값이 파일 이름. `segment.bytes` 옵션으로 크기 설정. 저장중인 세그먼트를 '액티브 세그먼트'라고 함.

대부분의 토픽이 `cleanup.policy`를 delete로 설정. 세그먼트 단위로 삭제. 삭제되면 복구 X

`retention.ms` - 밀리초 단위로 데이터 유지 기간 설정. 일정 시간마다 세그먼트 파일의 마지막 수정 시간과 retention.ms 비교, 넘어가면 세그먼트 삭제.

`retention.bytes` - 토픽의 최대 데이터 크기 제어

**2) 토픽 압축 정책(compact policy)**

메시지 키별로 해당 메시지 키의 레코드 중 오래된 것을 삭제. 카프카 스트림즈의 KTable과 같이 메시지 키를 기반으로 데이터 처리할 경우 유용. 

`min.cleanable.dirty.ratio` - 액티브 세그먼트 제외한 세그먼트에 남아있는 데이터의 테일(tail) 영역의 레코드 개수와 헤드(head) 영역의 레코드 개수 비율 

테일 영역 - 브로커의 압축 정책에 의해 압축이 완료된 레코드. 테일 영역의 레코드를 '클린 로그(clean)' 라 부름. 테일 영역엔 중복된 메시지 키 없다

헤드 영역 - 헤드 영역의 레코드들은 '더티(dirty) 로그'라고 부르고 중복된 메시지 키 있다. 

더티 비율(dirty ratio) - 더티 영역의 메시지 개수를 압축 대상 세그먼트에 남아있는 데이터의 총 레코드 수(더티 영역 + 클린 영역 메시지 개수)로 나눈 비율

min.cleanable.dirty.ratio 크게 설정하면 한번 압축할 때 많은 데이터 줄어들어 압축 효과 좋지만, 그 때까지 용량 많이 차지 → 용량 효율이 좋지 않음.  ↔ 작으면 압축 자주 일어나 최신 데이터만 유지 가능하지만 자주 압축하면 브로커에 부담

### 3. ISR(In-Sync-Replicas)

ISR: 리더 파티션과 팔로워 파티션이 모두 싱크가 된 상태

리더 파티션에 새로운 레코드가 추가되어 오프셋이 증가하면 팔로워 파티션이 위치한 브로커가 리더 파티션의 데이터 복제

`replica.lag.time.max.ms` - 팔로워 파티션에 데이터 복제하는지 확인하는 주기. 파티션이 이 시간보다 더 긴 시간동안 데이터를 가져가지 않으면 팔로워 파티션에 문제 생긴 것으로 판단, ISR 그룹에서 제외

ISR로 묶인 리더 파티션과 팔로워 파티션들은 파티션에 존재하는 데이터가 모두 동일하기 때문에 팔로워 파티션은 리더 파티션으로 선출될 자격을 갖춤

`unclean.leader.election.enable` - ISR이아닌 팔로워 파티션을 리더 파티션으로 선출 가능한지 여부. 토픽별로 성정 가능
false: ISR이 아닌 팔로워를 리더 파티션으로 선출 X, 리더 파티션이 있는 브로커 다시 시작할 때까지 대기 → 서비스 중단, 데이터 유실은 없음. 
true: ISR이 아닌 팔로워 파티션, 즉 동기화가 되지 않은 팔로워 파티션도 리더 선출 가능 → 동기화되지 않은 일부 데이터 유실, 서비스 중단 X.  
⇒ 운영 정책에 따라, 무중단 운영이 중요하면 true, 데이터 유실 안되면 false → 기다림

```java
bin/kafka-topics.sh --bootstrap-server [HOST]:9092 --create --topic [TOPIC] \
--confi unclean.leader.election.enable=false     // 토픽별로 설정 방법
```

## 카프카 프로듀서

### 1. acks 옵션

**acks=0**

프로듀서가 리더 파티션으로 데이터 전송했을 때 리더 파티션으로 데이터 저장됐는지 확인 X. 리더 파티션은 데이터 저장 이후 데이터 오프셋 리턴하는데 이것도 리턴 안함. 몇 번째 오프셋에 저장됐는지 모름.

프로듀서엔 전송 실패시 재시도 할 수 있도록 retries 옵션이 있는데 acks=0에선 데이터 보내자마자 성공 간주하므로 이게 의미가 없음

데이터 유실되어도 프로듀서는 리더 파티션으로부터 응답 받지않기 때문에 계속 보냄 → 데이터 전송 속도는 acks 가 1 or all 일 때보다 훨씬 빠름
→ 데이터 일부 유실되더라도 전송 속도가 중요한 경우

**acks=1**

프로듀서는 데이터가 리더 파티션에만 정상적으로 저장됐는지 확인. 리더 파티션에 적재되지 않았으면 재시도. 팔로워 파티션이 데이터 복제 직전에 리더 파티션 있는 브로커에 장애 생기면 유실 가능성 있음. acks=0 보다 전송 속도 느림. 적재 기다렸다 응답값 받기 때문에

**acks=all or acks=-1**

프로듀서는 리더 파티션과 팔로워 파티션에 모두 정상 적재되었는지 확인. acks=0, 1보다 느림. 일부 브로커에 장애 발생해도 팔로워 파티션에 데이터 정상 적재되었는지 확인하기 때문에 안전하게 전송, 저장 보장한다

all 옵션값은 모든 리더, 팔로워 파티션의 적재를 뜻하는 게 아니고 ISR에 포함된 파티션들을 의미.

`min.insync.replicas` - 프로듀서가 리더 파티션과 팔로워 파티션에 데이터 적재되었는지 확인하기 위한 최소 ISR 그룹의 파티션 개수. 값에 따라 데이터 안정성 달라짐. 토픽 단위로 설정. 1이라면 acks=1과 같은 동작. 처음 적재되는 파티션은 리더 파티션이기 때문. 
2부터 acks=all 이 의미가 있음. 2개 이상의 파티션에 정상 적재되었는지 확인한다는 뜻. 브로커 2개가 동시에 중단되는 일은 거의 없기 때문에 사실상 데이터 유실 X
카프카 브로커 개수 ≥ `min.insync.replicas` 이어야 함. 3으로 설정했는데 운영하는 3대 중 1개에 장애 발생 시 복제해야하는 브로커 개수 부족하여 NotEnoughReplicasAfterAppendException발생. 
따라서 절대로 브로커 개수와 동일한 숫자로 설정 X. 보통 토픽의 복제 개수 3, `min.insync.replicas`를 2, acks=all 로 설정

### 멱등성(idempotence) 프로듀서

멱등성 - 여러 번 연산 수행해도 동일한 결과를 나타내는 것

멱등성 프로듀서 - 동일한 데이터 여러 번 보내도 카프카 클러스터에 한 번만 저장

`enable.idempotence` - 정확히 한번 전달(exactly once delivery) 지원. 기본 false. true면 멱등성 프로듀서로 동작. 데이터 브로커로 전달할 때 프로듀서 PID(Producer Unique ID)와 시퀀스 넘버 전달, 브로커는 확인하여 동일만 메시지 적재 요청 오더라도 한 번만 데이터 적재

단, 동일한 세션에서만: PID의 생명주기: 애플리케이션 종료되고 재시작하면 달라짐 → 장애 발생하지 않을 경우에만 멱등성. 

`enable.idempotence` 를 true로 설정하면 몇가지 옵션 강제. 
`retries` - 프로듀서 데이터 재전송 횟수 Integer.MAX_VALUE 로 설정
`acks` - all 로 설정. 적어도 한 번 이상 브로커에 데이터 보내 단 한 번만 데이터 적재 보장 위해.

시퀀스 넘버 0부터 1씩 더한 값 전달되는데 이 값이 일정하지 않으면 OutOfOrderSequenceException 발생: 브로커가 예상한 시퀀스 넘버와 다른 번호의 데이터 적재 요청 왔을 때. 시퀀스 넘버의 역전현상 발생할 수 있음.

### 트랜잭션(transaction) 프로듀서

다수의 파티션에 데이터 저장할 경우 모든 데이터에 대해 동일한 원자성(atomic)을 만족시키기 위해 사용 → 다수의 데이터를 동일 트랜잭션으로 묶음, 전체 데이터를 처리하거나 전체 데이터를 처리하지 않음

`enable.idempotence` - true로 설정
`transactional.id` - 임의의 String 값으로 정의
컨슈머의 `isolation.level` - read_committed로 설정
⇒ 프로듀서와 컨슈머는 트랜잭션으로 처리 완료된 데이터만 쓰고 읽는다

트랜잭션 - 파티션의 레코드로 구분. 사용자가 보낸 데이터를 레코드로 파티션에 저장 + 트랜잭션 시작과 끝을 표현하기 위해 트랜잭션 레코드 하나 더 보냄. 트랜잭션 컨슈머는 트랜잭션 레코드 보고 트랜잭션 완료(commit)되었음 확인하고 데이터 가져감. 트랜잭션 레코드는 실질적인 데이터 없고 트랜잭션 끝난 상태 표시 정보만 갖고 있음, 오프셋 한 개 차지.

트랜잭션 컨슈머 - 커밋 완료된 데이터가 파티션에 있을때만 데이터 가져감

## 카프카 컨슈머

### 1. 멀티 스레드 컨슈머

자바 멀티 스레드 지원, 자바 애플리케이션에서는 멀티 스레드로 동작하는 멀티 컨슈머 스레드를 개발, 적용 가능

고려할 점: 한 프로세스 내부에 여러 개 스레드, 한 컨슈머 스레드에서 예외적 상황(ex. OutOfMemoryException) 발생하면 프로세스, 다른 컨슈머 스레드에 영향 → 중복 or 유실 발생 가능. 각 컨슈머 스레드 간에 영향 미치지 않도록 스레드 세이프 로직, 변수 등 적용

두 가지 방식
1. 멀티 워커 스레드 전략: 컨슈머 스레드 1개만 실행, 데이터 처리 담당하는 워커 스레드(worker thread) 여러 개 실행
2. 컨슈머 멀티 스레드 전략: 컨슈머 인스턴스에서 poll() 메서드 호출하는 스레드 여러 개 띄워서 사용 

**카프카 컨슈머 멀티 워커 스레드 전략**

멀티 스레드 생성하는 ExecutorService 자바 라이브러리 사용. Executors 사용하여 스레드 개수 제어하는 스레드 풀(thread pool) 생성. 스레드 종료해야 한다면 CachedThreadPool 사용하여 스레드 실행

- Runnable 인터페이스로 구현. 구현한 클래스는 스레드로 실행, 생성되고 나면 오버라이드된 run() 메서드 실행, 여기에 데이터 처리하는 구문. thread이름, record 클래스 로그 출력 등 가능
- 스레드 실행 위해 ExecutorService 사용 - 다양한 스레드 풀 제공
newCachedThreadPool - 필요한 만큼 스레드 풀 늘려서 스레드 실행하는 방식, 짧은 시간의 생명주기 가진 스레드에서 유용
- poll() 메서드 통해 리턴받은 레코드 처리하는 스레드를 레코드마다 개별 실행
- 스레드는 execute() 메서드로 실행
- 주의 사항
1. 스레드 사용함으로써 데이터 처리 안끝나도 커밋하기 때문에 리밸런싱, 컨슈머 장애 시에 데이터 유실 가능. 오토 커밋일 경우 데이터 처리가 스레드에서 진행 중임에도 불구하고 다음 poll() 메서드 호출 시에 커밋할 수도 있음.
2. 레코드 처리 역전현상. 스레드 생성은 순서대로지만 처리는 순서대로가 아닐 수도 있음.

**카프카 컨슈머 멀티 스레드 전략**

토픽의 파티션 개수만큼 컨슈머 스레드 개수 늘려서 운영

- Runnable 인터페이스로 구성
- KafkaConsumer 클래스는 스레드 세이프하지 않기 때문에 스레드별로 KafkaConsumer 인스턴스 별개로 만들어서 운영

### 컨슈머 랙

컨슈머 랙 - 토픽의 최신 오프셋(LOG-END-OFFSET)과 컨슈머 오프셋(CURRENT-OFFSET)간의 차이. 컨슈머 그룹과 토픽, 파티션별로 생성

**카프카 명령어를 사용하여 컨슈머 랙 조회**

```java
bin/kafka-consumer-groups.sh --bootstrap-server [HOST]:9092 \
-- group [GROUP] --describe  //  테스트용으로 주로 사용
```

**컨슈머 metrics() 메서드 사용하여 컨슈머 랙 조회**

KafkaConsumer 인스턴스의 metrics() 메서드 활용하여 컨슈머 지표 확인 가능

랙 관련 지표: records-lag-max, records-lag, records-lag-avg

문제점
1. 컨슈머 정상 동작할 때만 확인 
2. 모든 컨슈머 애플리케이션에 컨슈머 랙 모니터링 코드 중복해서 작성
3. 카프카 서드 파티 애플리케이션 모니터링 불가능

**외부 모니터링 툴 사용**

데이터 독(Datadog), 컨플루언트 컨트롤 센터, 버로우(Burrow) 등 카프카 클러스터 종합 모니터링 툴 사용. 컨슈머, 프로듀서 동작에 영향 미치지 않고 지표 수집

- 카프카 버로우 - 링크드인의 오픈소스 컨슈머 랙 체크 툴. 카프카 클러스터와 연동하여 REST API 통해 컨슈머 그룹별 컨슈머 랙 조회.
컨슈머 랙 평가 - 슬라이딩 윈도우 계산 통해 문제 생긴 파티션과 컨슈머 상태 표현. 파티션의 상태를 OK, STALLED, STOPPED로 표현, 컨슈머의 상태를 OK, WARNING, ERROR로 표현

### 컨슈머 배포 프로세스

짧은 시간 중단, 지연 발생해도 서비스 운영 괜찮으면 중단, 영향 크면 무중단 배포

**중단 배포**

컨슈머 애플리케이션 완전히 종료 후 개선된 코드 가진 애플리케이션 배포 

(+) : 신규 애플리케이션의 실행 전후를 명확하게 특정 오프셋 지점으로 나눌 수 있음. 버로우 사용하거나 배포 시점의 오프셋 로깅하면 신규 로직 적용 전후의 데이터를 명확히 구분 가능

**무중단 배포**

가상 서버 사용하는 경우에 유용.

블루/그린 배포 - 이전 버전, 신규 버전 애플리케이션 동시에 띄워놓고 트래픽 전환하는 방법. 파티션, 컨슈머 개수 동일하게 실행하는 애플리케이션에서 유용. 신규 버전 애플리케이션 준비되면 기존 애플리케이션 모두 중단. 리밸런스 한 번만 발생하기 때문에 많은 수의 파티션 운영하는 경우에도 짧은 리밸런스 시간으로 배포 수행

롤링 배포 - 인스턴스 할당, 반환으로 인한 리소스 낭비 줄이면서 무중단 배포. 파티션 개수가 인스턴스 개수와 같거나, 그보다 많아야 함. 인스턴스 개수만큼 리밸런싱 하기 때문에 파티션이 많지 않은 경우에 효과적

카나리 배포 - 많은 데이터 중 일부분을 신규 버전의 애플리케이션에 먼저 배포함으로써 이슈 없는지 사전에 탐지. ex) 100개 파티션 중 1개 파티션에 컨슈머 따로 배정하여 테스트 후, 사전 테스트 완료되면 나머지는 롤링 or 블루/그린 배포하여 무중단 배포 가능