# 카프카 스트림즈, 카프카 커넥트

## 카프카 스트림즈

- 토픽에 적재된 데이터를 실시간으로 변환하여 다른 토픽에 적재하는 라이브러리
- 카프카 클러스터와 완벽하게 호환, 신규 토픽 생성, 상태 저장, 데이터 조인 등 각종 스트림 처리에 필요한 기능들 제공
- Exactly Once 데이터 처리, Fault Tolerant System

- 소스 토픽과 싱크 토픽의 클러스터가 서로 다른 경우처럼 스트림즈가 지원하는 않는 경우는 컨슈머와 프로듀서의 조합으로 직접 개발

**스레드, 태스크**

1개 이상 스레드 생성할 수 있으며, 스레드는 1개 이상의 태스크 가짐

태스크 - 데이터 최소 처리 단위.  토픽의 3개 파티션 → 3개 태스크

실제 운영환경에선 장애 발생을 고려하여 2개 이상의 서버로 구성하여 스트림즈 어플리케이션 운영

**토폴로지**

![%E1%84%8F%E1%85%A1%E1%84%91%E1%85%B3%E1%84%8F%E1%85%A1%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%E1%84%85%E1%85%B5%E1%86%B7%E1%84%8C%E1%85%B3,%20%E1%84%8F%E1%85%A1%E1%84%91%E1%85%B3%E1%84%8F%E1%85%A1%20%E1%84%8F%E1%85%A5%E1%84%82%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20f7cf7ba4af6943b095bae7869252a004/Untitled.png](%E1%84%8F%E1%85%A1%E1%84%91%E1%85%B3%E1%84%8F%E1%85%A1%20%E1%84%89%E1%85%B3%E1%84%90%E1%85%B3%E1%84%85%E1%85%B5%E1%86%B7%E1%84%8C%E1%85%B3,%20%E1%84%8F%E1%85%A1%E1%84%91%E1%85%B3%E1%84%8F%E1%85%A1%20%E1%84%8F%E1%85%A5%E1%84%82%E1%85%A6%E1%86%A8%E1%84%90%E1%85%B3%20f7cf7ba4af6943b095bae7869252a004/Untitled.png)

트리 형태의 토폴로지.

프로세서 - 노드
1. 소스 프로세서 - 데이터 처리 위해 최초로 선언해야 하는 노드. 하나 이상의 토픽에서 데이터 가져오는 역할
2. 스트림 프로세서 - 다른 프로세서가 반환한 데이터 처리하는 역할. 변환, 분기처리와 같은 로직 등
3. 싱크 프로세서 - 데이터를 특정 카프카 토픽으로 저장하는 역할. 스트림즈로 처리된 데이터의 최종 종착지

스트림 - 간선. 토픽의 데이터를 의미(프로듀서, 컨슈머의 레코드와 동일)

- 스트림즈 DSL or 프로세서 API 2가지 방법으로 개발 가능

스트림즈 DSL - 스트림 프로세싱에 쓰일만한 다양한 기능들을 자체 API로 만들어 놓아 대부분의 변환 로직 어렵지 않게 개발할 수 있다. 

### 스트림즈 DSL

**KStream**

- 레코드의 흐름을 표현한 것. 메시키 키와 메시지 값으로 구성.
- KStream으로 데이터 조회하면 토픽에 존재하는(or KStream에 존재하는) 모든 레코드 출력
- 컨슈머로 토픽 구독하는 것과 동일한 선상

**KTable**

- 메시지 키 기준으로 묶어서 사용
- KStream이 모든 레코드 조회할 수 있다면, KTable은 유니크한 키 값 기준으로 가장 최신에 추가된 레코드의 데이터 출력

**GlobalKTable**

- 메시지 키 기준으로 묶어서 사용 (KTable과 동일하게)
- KTable로 선언된 토픽은 1개 파티션이 1개 태스크에 할당되어 사용 vs GlobalKTable로 선언된 토픽은 모든 파티션 데이터가 각 태스크에 할당되어 사용

※ Co-partitioning (코파티셔닝)

 - 조인을 하는 2개 데이터의 파티션 개수가 동일하고 파티셔닝 전략을 동일하게 맞추는 것

KStream과 KTable 데이터를 조인할 때 코파티셔닝되어 있지 않으면 `TopologyException` 발생 ⇒ 리파티셔닝(repartitioning) 과정 거쳐야 함 

코파티셔닝 되어있다면 KStream의 레코드와 KTable의 메시지 키가 동일할 경우 조인 수행

※ Repartitioning (리파티셔닝)

- 새로운 토픽에 새로운 메시지 키 가지도록 재배열하는 과정

- 기존 데이터 중복해서 생성할 뿐만 아니라 파티션 재배열하기 위해 프로세싱하는 과정도 거침

⇒ 해결방안: 코파티셔닝되지 않은 KStream과 KTable 조인하고 싶다면 KTable을 GlobalKTable로 선언하여 사용. GlobalKTable로 정의된 데이터는 스트림즈 애플리케이션의 모든 태스크에 동일하게 공유되어 사용되기 때문.  
But, 태스크마다 GlobalKTable로 정의된 모든 데이터 저장하고 사용하기 때문에 오버헤드가 큼. 작은 용량의 데이터일 경우에만 사용하는 것이 좋음. 많을 땐 리파티셔닝을 통해 KTable 사용 권장.

**스트림즈DSL 주요 옵션**

필수 옵션

- `bootstrap.servers` - 브로커의 이름:포트를 1개 이상 작성
- `application.id` - 스트림즈 애플리케이션 구분 위한 고유 ID

선택 옵션

- default.key.serde - 메시지 키 직렬화, 역직렬화하는 클래스 지정. 기본 `Serdes.ByteArray().getClass().getName()` → 바이트 직렬화, 역직렬화 클래스
- default.value.serde - 메시지 값 직렬화, 역직렬화하는 클래스 지정. 기본 위와 동일
- num.stream.threads - 스트림 프로세싱 실행 시 실행될 스레드 개수 지정. 기본 1
- state.dir - 상태기반 데이터 처리 시 데이터 저장할 디렉토리. 기본 `/tmp/kafka-streams`

**스트림즈DSL** 

stream() - 특정 토픽을 KStream 형태로 가져오기

to() - KStream의 데이터를 특정 토픽으로 저장

filter() - 메시지 키 또는 값을 필터링하여 특정 조건에 맞는 데이터만 골라내기 → 필터링 스트림 프로세서. 자바의 함수형 인터페이스인 Predicate를 파라미터로 받음ex) (key, value) -> value.length() > 5  

KTable과 KStream을 join() - 메시지 키를 기준으로 조인
- 실시간으로 들어오는 데이터 조인 가능 → 사용자의 이벤트 데이터를 DB에 저장하지 않고도 조인하여 스트리밍 처리할 수 있다
- 코파티셔닝 되어있어야 한다. 파티션 개수, 파티셔닝 전략 동일하게

```java
orderStream.join(addressTable, // KStream을 KTable과 조인
				(order, address) -> order + "send to" + address) // 키 값을 기준으로 매칭
				.to(ORDER_JOIN_STREAM) // 새로운 토픽으로 
```

GlobalKTable과 KStream을 join() - 코파티셔닝 되어있지 않아도 가능. But, 토픽에 존재하는 모든 데이터를 태스크마다 저장하고 조인 수행. 키 뿐만 아니라 메시지 값을 기준으로도 매칭하여 조인 가능.

```java
orderStream.join(addressGlobalTable, // KStream을 GlobalKTable과 조인 (오버로딩)
				(orderKey, orderValue) -> orderKey, // 매칭 조건을 키, 밸류 다 사용 가능
				(order, address) -> order + "send to" + address) // 위에서 정한 키 기준으로 매칭
				.to(ORDER_JOIN_STREAM) // 새로운 토픽으로 
```

### 프로세서 API

- KStream, KTable, GlobalKTable 등의 개념이 없음

```java
import org.apache.kafka.streams.processor.ProcessorContext;

import org.apache.kafka.streams.processor.Processor;

public class FilterProcessor implements Processor<String, String> {
// kafka-streams 라이브러리의 Processor 인터페이스 상속
    private ProcessorContext context;

    @Override
    public void init(ProcessorContext context) {
        this.context = context;
    }
	  // ProcessorContext 클래스 - 프로세서에 대한 정보. 생성된 인스턴스로 현재 
		// 스트림 처리중인 토폴로지의 토픽 정보, 애플리케이션 아이디 조회 
		// schedule(), forward(), commit() 등의 프로세스 처리에 필요한 메서드
		
		// init() - 스트림 프로세서 생성자. 프로세싱 처리에 필요한 리소스 선언
    @Override
    public void process(String key, String value) {
        if (value.length() > 5) {
            context.forward(key, value);
        }
        context.commit();
    }
		// 1개의 레코드 가정하고 프로세싱 로직 작성 메시지 키, 값을 파라미터로
		// 필터링 후 forward()로 다음 토폴로지로
		// 처리 완료 이후 commit() 호출하여 데이터 처리되었음 선언
    @Override
    public void close() {
    }
		// 종료되기 전에 호출되는 메서드. 리소스 해제하는 구문
}
```

## 카프카 커넥트

- 반복 작업을 줄이고 효율적인 전송을 이루기 위한 애플리케이션
- 특정한 작업 형태를 템플릿으로 만들어놓은 커넥터(connector) 실행함으로써 반복 작업 줄인다
- 소스 커넥터 - 프로듀서 역할
- 싱크 커넥터 - 컨슈머 역할
- HDFS, AWS S3, JDBC, ElasticSearch 커넥터 등 오픈소스 커넥터들이 많다

**Basic**

- 커넥트에 커넥터 생성 명령 내리면 커넥트는 내부에 커넥터와 태스크 생성, 커넥터는 태스크들을 관리

- 태스크: 커넥터에 종속되는 개념. 실질적인 데이터 처리

- 컨버터(converter): 데이터 처리 전에 스키마 변경하도록 도와줌. JsonCoverter, StringConverter, ByteArrayConverter 지원

- 트랜스폼(transform): 데이터 처리 시 각 메시지 단위로 데이터를 간단하게 변환하기 위한 용도로 사용. 기본 Cast, Drop, ExtractField 지원

**커넥트 실행 방법**

1. 단일 모드 커넥트(standalone mode kafka connect) - 단일 애플리케이션. 커넥터 정의하는 파일 작성하고 해당 파일 참조하는 단일 모드 커넥트 실행함으로써 파이프라인 생성
- 단일 프로세스이기 때문에 단일 장애점(SPOF: Singel Point Of Failure)이 될 수 있음
- 따라서 주로 개발환경, 중요도 낮은 파이프라인 운영 시에 사용
2. 분산 모드 커넥트(distributed mode kafka connect) - 2대 이상의 서버에서 클러스터 형태로 운영
- 장애 발생해도 남은 커넥트가 파이프라인 처리 → 안전
- 데이터 처리량 변화에도 유연하게 대응 → 서버 개수 늘려 처리량 늘림

REST API로 현재 실행중인 커넥트의 커넥터 플러그인 종류, 태스크 상태, 커넥터 상태 등 조회 가능(p.158 참조)

**단일 모드 커넥트**

config 디렉토리의 connect-standalone.properties 설정파일 수정

```java
bootstrap.servers=my-kafka:9092 // 커넥트와 연동할 클러스터

key.converter=org.apache.kafka.connect.json.JsonConverter // 카프카에서 가져오거나 저장시
value.converter=org.apache.kafka.connect.json.JsonConverter // 데이터 변환에 사용
key.converter.schemas.enable=false // 스키마 사용하고 싶지 않을 때
value.converter.schemas.enable=false

offset.storage.file.filename=/tmp/connect.offsets
// 단일 모드 커넥트는 로컬 파일에 오프셋 정보 저장
offset.flush.interval.ms=10000 // 오프셋 커밋 주기

plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins
// 플러그인 형태로 추가할 커넥터의 디렉토리 주소
// 오픈소스 or 직접 개발한 커넥터의 jar파일이 위치하는 디렉토리
```

파일 소스 커넥터 설정파일: config/connect-file-source.properties

```java
name=local-file-source // 커넥터 이름
connector.class=FileStreamSource // 커넥터 클래스 이름. FileStreamSource: 기본 제공
tasks.max=1 // 커넥터로 실행할 태스크 개수
file=/tmp/test.txt // 읽을 파일 위치
topic=connect-test // 데이터 저장할 토픽의 이름
```

실행

```java
bin/connect-standalone.sh config/connect-standalone.properties\
config/connect-file-source.properties
```

**분산 모드 커넥트**

2개 이상의 프로세스가 1개의 그룹으로 묶여서 운영

```java
bootstrap.servers=my-kafka:9092 // 클러스터
group.id=connect-cluster // 다수의 커넥트 프로세스들을 묶을 그룹 이름
// 같은 그룹으로 지정된 커넥트들에서 커넥터 실행되면 커넥트들에 분산되어 실행

key.converter=org.apache.kafka.connect.json.JsonConverter // 카프카에서 가져오거나 저장시
value.converter=org.apache.kafka.connect.json.JsonConverter // 데이터 변환에 사용
key.converter.schemas.enable=false // 스키마 사용하고 싶지 않을 때
value.converter.schemas.enable=false

offset.storage.topic=connect-offsets
offset.storage.replication.factor=1
config.storage.topic=connect-configs
config.storage.replication.factor=1
status.storage.topic=connect-status
status.storage.replcation.factor=1
// 분산 모드 커넥트는 카프카 내부 토픽에 오프셋 정보 저장
// 실제 운영시 3이상으로 설정 권장

offset.flush.interval.ms=10000 // 오프셋 커밋 주기

plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins
```

실행

```java
bin/connect-distributed.sh config/connect-distributed.properties
```

### 소스 커넥터

소스 애플리케이션 또는 소스 파일로부터 데이터를 가져와 토픽으로 넣는 역할

기본 제공하는 SourceConnector, SourceTask 클래스 사용하여 직접 소스 커넥터 구현 가능, jar 파일로 만들어 플러그인으로 추가하여 사용

SourceConnector - 태스크 실행 전 커넥터 설정파일 초기화하고 어떤 태스크 클래스 사용할지 정의. 실질적인 데이터 다루는 부분 들어가지 않음

SourceTask - 소스 애플리케이션or파일에서 데이터 가져와 토픽으로 데이터 보내는 역할. 자체적으로 사용하는 오프셋. 파일 or 애플리케이션 어디까지 읽었는지 → 중복 방지

### 싱크 커넥터

토픽의 데이터를 타깃 애플리케이션 or 파일로 저장하는 역할

SinkConnector - 태스크 실행 전 사용자로부터 입력받은 설정 초기화하고 어떤 태스크 클래스 사용할 것인지 정의. 실질적인 데이터 로직 X

SinkTask - 컨슈머 역할, 데이터 저장하는 코드 가짐

# 카프카 미러메이커2

- 서로 다른 카프카 클러스터 간에 토픽을 복제하는 애플리케이션
- 메시지 키, 값, 파티션 정보 등 토픽의 모든 것을 복제
- 복제하는 토픽의 파티션 개수 달라지면 따라서 달라져야 하기 때문에 어드민까지 조합 → 관련된 모든 기능 지원
- 파티션 정보가 다르고 데이터 유실, 중복이 발생하는 등 미러메이커1의 단점 해소
- 토픽의 데이터 복제할 뿐만 아니라 토픽 설정까지도 복제하여 파티션의 변화, 토픽 설정값의 변화도 동기화하는 기능
- 커넥터 사용 가능 - 분산 모드 커넥트 운영한다면 커넥트에서 미러메이커2 커넥터를 실행하여 토픽 복제

**단방향 토픽 복제**

config 디렉토리의 `connect-mirror-maker.properties` 수정

```java
clusters = A,B
A.bootstrap.servers = a-kafka:9092
B.bootstrap.servers = b-kafka:9092

A->B.enabled = true  // 단방향
A->B.topics = test

B->A.enabled = false
B->A.topics = .*

replication.factor = 1 // 신규 생성된 토픽의 복제 개수

checkpoints.topic.replication.factor = 1
heartbeats.topic.replication.factor = 1
offset-syncs.topic.replication.factor = 1
offset.storage.replication.factor = 1
status.storage.replication.factor = 1
config.storage.replication.factor = 1   // 토픽 복제에 필요한 데이터 저장하는
																				// 내부 토픽의 복제 개수
```

```java
bin/connect-mirror-maker.sh config/connect-mirrormaker.properties
```

### 지리적 복제(Geo-Replication)

**액티브-스탠바이 클러스터 운영** 

- 액티브 클러스터(Active) - 서비스 애플리케이션들이 직접 통신하는 카프카 클러스터
- 스탠바이 클러스터(Standby) - 재해 복구를 위해 임시로 구성한 클러스터

자연재해, 기술적 재해, 인간에 의한 재해에 대응하여 물리적인 공간 분리 

**액티브-액티브 클러스터 운영**

두 개의 클러스터 두면서 서로 미러링 - 애플리케이션 통신 지연 최소화

다른 지역에서 각각 서로 미러링하면서 운영

**허브 앤 스포크(Hub and spoke) 클러스터 운영**

각 팀에서 소규모 카프카 클러스터 운영

- 허브 - 데이터 레이크의 역할. 데이터 수집, 가공, 분석하는 격리된 플랫폼
- 스포크 - 소규모 카프카 클러스터